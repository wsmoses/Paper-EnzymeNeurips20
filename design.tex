\section{Design}
\label{sec:design}
Though it is clear that synthesizing gradients is better done after optimization, doing so requires working at a lower-level of abstraction which presents additional challenges.

Enzyme works in the following stages:
\begin{itemize}
    \item Analyze types and instruction activity
    \item For all active values, allocate and zero memory to create corresponding shadow values.
    \item For each Basicblock \texttt{B}, emit the adjoint for all instruction of \texttt{B} in reverse order
    \item Optimize
\end{itemize}


\AtBeginEnvironment{minted}{%
  \renewcommand{\fcolorbox}[4][]{#4}}



\newminted[codebox]{llvm}{texcl=true, autogobble=true,breaklines}

\begin{figure}
    \centering
\begin{tikzpicture}
\node[inner sep=0, outer sep=0] (orig) {
\begin{minipage}[T]{0.49\linewidth}
\begin{codebox}
define double @reluf(double %x)
entry:
  ; Shadow values for reverse
  ; alloca %d\_x = 0.0
  ; alloca %d\_call = 0.0
  ; alloca %d\_result = 0.0
  br (%x > 0), if.true, if.end
if.true:
  %call = f(%x)
  br cond.end
if.end:
  %res = phi[%call, if.true],      [0, entry]
  ret %res
\end{codebox}
\end{minipage}
};

\node[inner sep=0, outer sep=0, xshift=6.2cm] (reverse) {
\begin{minipage}[T]{0.49\linewidth}
\begin{minted}[fontsize=\small]{llvm}
reverse_if.end:
  ; adjoint of return
  store %d_result = 1.0
  ; adjoint of %result phi node
  %d_call += if %x > 0.0, (load %d_result), else 0
  store %d_result = 0.0
  br %cmp, %reverse_if.true, %reverse_entry
reverse_if.true:
  ; adjoint of %call
  %df = @diffe_f(%x)
  %d_x += %df * (load %call)
  store %d_call = 0.0
  br %reverse_entry
reverse_entry:
  %0 = load %d_x
  ret %0
\end{minted}
\end{minipage}
};
\end{tikzpicture}
    \caption{Example differential reverse creation for \texttt{relu(f(x))}. The left hand side shows the LLVM IR for the original computation, with commented out shadow allocations of active variables. The right hand side shows the reverse pass that Enzyme would generate to propagate gradient information across the program.}
    \label{fig:reluf}
\end{figure}

\begin{figure}
\centering
\begin{minted}[fontsize=\small]{c}
double sum(double* x) {
  double total = 0;
  for(int i=0; i<10; i++)
    total += read() * x[i];
  return total;
}

void diffe_sum(double* x, double* xp) {
  return __enzyme_autodiff(sum, x, xp);
}

void diffe_sum(double* x, double* xp) {
  double* read_cache = malloc(10*8);
  for(int i=0; i<10; i++)
    readCache[i] = read();
  // reverse
  for(int i=10-1; i>=0; i--)
    xp[i] += read_cache[i];
  }
  free(read_cache);
}
\end{minted}
\caption{Example of caching the result of read for the reverse pass and allocations for loops.}
\label{fig:cacheloop}
\end{figure}

\begin{figure}
\begin{minted}[fontsize=\small]{c}
double loadsq(double* x) {
  return x[0] * x[0];
}
void f(double* x) {
  *x = loadsq(x);
}

{double,double} augmented_loadsq(double* x) {
  return {/*return val*/x[0]*x[0], /*cache*/x[0] };
}

void diffe_loadsq(double* x, double* d_x, double d_ret, double cache) {
  d_x[0] += 2 * cache * d_ret;
}

void diffe_f(double* x, double* xp) {
  {call, cache} = augmented_loadsq(x);
  *x = call;
  double d_ret = *d_x;
  *d_x = 0;
  diffe_loadsq(x, d_x, d_ret, cache);
}
\end{minted}
\caption{Creating an augmented forward pass for a function to ensure requisite values are cached for the reverse.}
\label{fig:cacheaugment}
\end{figure}

\subsection{Shadow Memory}
For every active value in the original program, Enzyme creates and zero's a shadow version of that value. 

Local data structures with active variable need to be
duplicated to store derivative information
* Leverage all data structures are created by specific
memory instructions (malloc/free/new/delete/etc)
* Allocations are copied in forward pass to create
differential structures
* Frees are delayed until the reversed version of the
block that allocated in case values are used in the
reverse pass

\subsection{Type Analysis}

Types inside LLVM IR (and even C/C++) do not necessarily represent the type of the underlying data. For example, the \texttt{memcpy} function which copies data operates on generic pointers without types (\texttt{void*}). Creating a correct gradient for \texttt{memcpy}, however, requires knowing the type of the memory being copied. If you were copying 8 bytes of double data you would perform one double (8-byte) addition in the reverse pass, whereas if you were copying 8 bytes of float data you would need to do two float (4-byte) additions.

Taking the derivative of instructions such as \texttt{memcpy} depends on the type of the data being copied. As demonstrated in Figure \ref{fig:memcpy}, the derivative of a \texttt{memcpy} of memory containing doubles should add the propagate differential destination into the differential source, whereas a \texttt{memcpy} of pointer data should copy the differential source pointers into the differential destination in the forward pass, with nothing in the reverse. As these two operations are incompatible with each other (resulting in garbage data if the wrong one is used), we must be able to determine the type of the data being copied.

We do so by creating a new interprocedural type analysis. We give every value in a function a \define{type tree} that describes the known type at any given byte offset in the value. If the type at a particular offset is a pointer type, we have a new type tree that represents the types inside that offset. As an example the type tree \verb|{[]:Pointer, [0]:Double, [8]:Integer}| represents a pointer to a struct that contains first a double at byte 0, then an integer at byte 8.

Type analysis starts by initializing the type trees of all values to empty. Type based alias analysis (TBAA) is a type of metadata in LLVM that guarantees that the type of the data must be a specified type because of strict aliasing. Type analysis uses TBAA metadata to initialize its type trees for load and stores. For every type of instruction, type analysis implements a propagation rule specifying how types propagate forward and backward through that instruction. As an example, if the result of a load is known to be type T, then the pointer loaded must be a pointer to T at offset 0. Type analysis then runs all of the type propagation until a fixed point is reached.

\begin{figure*}
    \centering
\begin{minted}[fontsize=\small]{c}
void f(void* dst, void* src) { memcpy(dst, src, 8); }
\end{minted}
\begin{tabular}{c|c}
\begin{minipage}[T]{0.49\linewidth}
\begin{minted}[fontsize=\small]{c}
// Gradient memcpy for double inputs
void grad_f(double* dst, double* ddst,
            double* src, double* dsrc) {
  // Forward
  memcpy(dst, src, 8);
  // Reverse
  dsrc[0] += ddst[0];
  ddst[0] = 0;
  
  
}
\end{minted}
\end{minipage}&
\begin{minipage}[T]{0.49\linewidth}
\begin{minted}[fontsize=\small]{c}
// Gradient memcpy for float inputs
void grad_f(float* dst, float* ddst,
            float* src, float* dsrc) {
  // Forward
  memcpy(dst, src, 8);
  // Reverse
  dsrc[0] += ddst[0];
  ddst[0] = 0;
  dsrc[1] += ddst[1];
  ddst[1] = 0;
}
\end{minted}
\end{minipage}
\end{tabular}
\caption{Memcpy}
\label{fig:memcpy}
\end{figure*}

\subsection{Activity Analysis}
Activity analysis is common in automatic differentiation systems to avoid taking gradients of instructions that don't impact the gradient computation. We also use it to avoid taking gradients of instructions for which that would be ill defined (e.g. what is the gradient of adding two integers). An instruction is active iff it takes a differential value and can propagate it to its return or another memory location. For example a function that counts the length of a an active input array would not be active. In our implementation of activity analysis, we leverage alias analysis and type analysis to prove that more instructions are inactive. As an example, any read-only function that returns an integer must be inactive.

\subsection{Combined Forward-Reverse Analysis}
Whenever possible, it is desirable to compute both the forward and reverse pass in the same function. This allows values to be optimized between the forward and reverse pass, and can reduce the memory usage.% as the cache for values inside 
This analysis detects whether it is legal to execute the forward pass instructions of a function where the gradient computation would occur n reverse pass. If so the forward pass call is erased and the combined function is simply executed in the reverse.

\subsection{Cache}
We need to cache several values for the reverse pass as it may be illegal to recompute them. For example, calling a function that reads for standard in or reading from memory that is overwritten. 
\todo{talk about alias analysis here}

\subsection{Differential Use Analysis}
To maximize performance, it is desirable to minimize the number of values cached. We derive which values are not necessary for computing the gradient and avoid caching them.

\subsection{Already-cached Analysis}
If we've already cached an equivalent value (e.g. a load to the same location), use the existing cache for the value.

\subsection{Minimal Cache}
If a cached value is only used to recompute a single value, we might as well cache the value being recomputed, rather than the original value, preventing the need to recompute it in the reverse pass.
\todo{interesting graph theory extension/problem here, perhaps in conclusion}

Very minor: combine 8 bools into byte
