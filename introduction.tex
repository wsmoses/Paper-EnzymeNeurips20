\section{Introduction}
\label{sec:intro}

Machine learning frameworks such as PyTorch~\cite{paszke2017automatic} and Tensorflow~\cite{abadi2016tensorflow} have become widespread as the primary workhorses of the modern machine learning community. In order to compute the necessary gradients for algorithms such as backpropagation~\cite{hecht1992theory}, bayesian inference, and uncertainty quantification~\cite{Wang2018-yr}, however, programmers are required to rewrite all of their code to be in said framework. This rewriting is especially problematic for applying machine learning to new domains as existing tools like physics simulators~\cite{feng2016fastpm, broughton2020tensorflow, NIPS2018_7948, degrave2019differentiable, hu2019difftaichi}, game engines, climate models~\cite{Stevens2020-ir}, and medical models~\cite{alquraishi2019end} that are not written in the domain specific languages of machine learning frameworks. High performance automatic differentiation is also necessary for probabilistic programming frameworks~\cite{cusumano2019gen}.

This rewriting has been identified as the quintessential challenge of applying machine learning to scientific computing~\cite{Baker2019-ty}. As stated in \citet{10.15200/winn.156631.13064} ``This is [the key challenge of scientific ML] because, if there is just one part of your loss function that isn't AD-compatible, then the whole network won't train.''

To remedy this issue, the trend has been to create new domain-specific languages (DSL) \todo{hu2019difftaichi, more citations} that make the rewriting process easier or to integrate differentiation as a first-class construct in programming languages to enable differential programming ($\delta P$)~\cite{maclaurin2015autograd,jax2018github,SwiftAutodiff,zygoteMlsys}. This results in efficient gradients, but still requires rewriting in either the DSL or the differential programming language. 


To use existing code that is foreign to the Machine Learning framework developers can use reverse-mode AD frameworks that operate on the foreign language. 

\todo{You don't have to write complicated losses in tensorflow math world;}

In languages without an existing first-class AD framework, there have been two approaches to computing gradients.

\define{Operator-overloading} approaches take advantage existing language functionality compute derivatives of functions alongside the original code. Examples of this include Adept~\cite{adept} and ADOL-C~\cite{griewank1996algorithm}, C++ libraries that provide differentiable versions of primitive types or JaX~\cite{jax2018github} and AutoGrad~\cite{maclaurin2015autograd} are Python libraries that provides derivatives NumPy-style functions. These approaches, however, require rewriting your source code to use the differentiable types/operations in place of standard language utilities and thus also cannot differentiate many libraries, nor any code in other languages.
% , and can often be slow as they must store what operations are taking place in order to later differentiate.

\define{Source-rewriting}~\cite{10.5555/1455489} approaches analyze the source code of programs and emit new source code that computes the derivative of a function. Examples include Tapenade~\cite{TapenadeRef13}, a framework which computes forward and backwards mode derivatives for C and Fortran programs; ADIC~\cite{narayanan2010adic2} another source-rewriter for C and C++; and Zygote~\cite{zygoteArxiv,zygoteDP,zygoteMlsys}, a framework that dynamically generate derivative functions in Julia. Such tools can be more efficient than operator-overloading approaches as they can statically analyze the computation. They also tend to only work with programs in a specific language, sometimes even only a subset of programs in that language. They also require all of the source to be differentiated to be available to the tool. This makes them difficult to use with header-only libraries and impossible to use with precompiled libraries.

Moreover, as more codebases use libraries from other languages, it is necessary to compute gradients of functions from many languages in the same framework. Tapenade is able to do this between C and Fortran~\cite{pascual2016mixed}.\todo{discuss other cross language AD}

\begin{figure*}
    \centering
\begin{minted}[fontsize=\small]{c}
double mag(const double* x);//Compute magnitude in O(N)
void norm(double* out, double* in) {
    // double res = mag(in); code motion optimization can move outside the loop
    for(int i=0; i<N; i++) { out[i] = in[i]/mag(in); }
}
\end{minted}
\begin{tabular}{c|c}
\begin{minipage}[T]{0.49\linewidth}
\begin{minted}[fontsize=\small]{c}
// LICM, then AD, O(N)
void norm(double* out, double* dout,
         double* in, double* din) {
  double res = mag(in);
  for(int i=0; i<N; i++) {
    out[i] = in[i]/res;
  }
  double dres = 0;
  for(int i=0; i<N; i++) {
    dres += -in[i]*in[i]/mag * dout[i];
    din[i] += dout[i]/mag;
  }
  dmag(in, din, dres);
}
\end{minted}
\end{minipage}& \begin{minipage}[T]{0.49\linewidth}
\begin{minted}[fontsize=\small]{c}
// AD, then LICM O(N^2)
void norm(double* out, double* dout,
          double* in, double* din) {
  double res = mag(in, n);
  for(int i=0; i<n; i++) {
    out[i] = in[i]/res;
  }
  for(int i=0; i<n; i++) {
    double dres = -in[i]*in[i]/mag \
                        * dout[i];
    din[i] += dout[i]/mag;
    dmag(in, din, n, dres);
  }
}
\end{minted}
\end{minipage}
\end{tabular}
    \caption{In the second program, mag is still able to be moved outside as it is the same every iteration, however, \texttt{dmag} cannot be moved outside the loop as it reads/writes to the same memory.
}
    \label{fig:licm}
\end{figure*}

Both operator-overloading and source-rewriting AD systems differentiate programs before optimization. Optimizations can  significantly reduce the work in a program by deleting and simplifying instructions. Performing AD on an unoptimized program requires will create longer and more complex backwards passes, which may not be optimizable. Consider the program at the top of Figure~\ref{fig:licm} that normalizes a vector. A smart compiler will move the $O(N)$ call to \texttt{mag} outside the loop, changing the runtime of the code from $O(N^2)$ to $O(N)$ by an optimization known as loop-invariant-code-motion (LICM)~\cite[Sec.~13.2]{Muchnick97}. Running LICM then AD results in the $O(N)$ code on the left with both \texttt{mag} and its adjoint \texttt{dmag} are outside the loop. However, if AD is run before LICM, the forward pass will have \texttt{mag} outside the loop, however AD will call the adjoint \texttt{dmag} inside the loop. LICM cannot move \texttt{dmag} outside the loop as it uses a value computed inside the loop.

Traditional AD systems have not been able to operate on optimized IR as it either requires re-implementing all of the compiler optimizations or working at a low level after which optimization has already been performed. It has conventionally believed to be hard to produce efficient gradients for low-level code as it lacks high-level information many AD tools rely upon. As described in \citet{zygoteArxiv}, ```AD is more effective in high-level compiled languages (e.g. Julia, Swift, Rust, Nim) than traditional ones such as C/C++, Fortran and LLVM IR [...].''' The LLVM~\cite{LLVM} compiler toolchain defines a low level intermediate representation (IR) and provides a set of optimizations used by many compiler toolchains.

This paper presents Enzyme, an efficient cross-platform compiler plugin for automatic differentiation that operates on LLVM IR~\cite{LLVM} and makes the following contributions:
\begin{itemize}

\item Enzyme, a compiler plugin for LLVM that can synthesize high-performing gradients of statically analyzable LLVM IR, including IR generated by compiler frontends for C, C++, Fortran, Nim, Rust, Swift, and others.

\item PyTorch-Enzyme, a foreign-function interface for the PyTorch machine learning framework that allows programmers to use code written in LLVM-compiled languages in their machine-learning workflows, as well as a similar foreign-function interface for TensorFlow.

\item Enzyme.jl, a package that uses Enzyme to synthesize high-performing gradients of Julia code, which demonstrates that gradients of code written in a dynamic high-level language can be generated statically and perform well with only low-level information.

\item A study demonstrating that running AD after optimization results in significant performance gains on a standard machine learning benchmark suite~\cite{adBench} and achieve state-of-the-art performance with other AD systems.

\todo{thoughts of multisource or clang native AD}
\end{itemize}

\paragraph{Related work}

Clad is a plugin to the Clang compiler that implements forward automatic differentiation on a subset of C/C++ with reverse mode in development~\cite{Vassilev_Clad}. \citet{chen2018neural} present an end-to-end differentiable model for protein structure prediction. 
DiffTaichi~\cite{hu2019difftaichi} implements a differential DSL for physics and robotics simulation. \citet{NIPS2018_7948} also provides a differential physics framework. Halide is a differentiable DSL for image processing~\cite{Li:2018:DPI}. Swift implements first class automatic differntiation~\cite{SwiftAutodiff}. A compiler plugin to provide differentiable programming in Haskell~\cite{Elliott2018-gr}.

